# **🚀 Understanding Word Embeddings, Vector Spaces, and UMAP**  

Welcome to this **interactive and beginner-friendly** series of lessons on **word embeddings, vector spaces, and dimensionality reduction with UMAP!** 🎉  

These lessons will help you **understand how machines represent words** as numbers, **compare words mathematically**, and **visualize high-dimensional data** in a simple way.

---

## **📌 Lessons Overview**

### **1️⃣ Word Embeddings & Cosine Similarity**  
✅ Learn how words can be represented as **vectors** in a high-dimensional space  
✅ Find **similar words** using **pre-trained Word2Vec/GloVe models**  
✅ **Visualize** words in a 2D space using **PCA**  
✅ Measure **word similarity** using **cosine similarity**  

📌 **Key Skills:** Word2Vec, GloVe, cosine similarity, vector mathematics  

---

### **2️⃣ Word Vectors, Vector Spaces & Custom Cosine Similarity**  
✅ Create your **own word vectors** by assigning words x, y (or x, y, z) coordinates  
✅ **Visualize words in 2D & 3D** using Matplotlib  
✅ Compute **cosine similarity manually** to see how close words are  
✅ Experiment with **different words and dimensions**  

📌 **Key Skills:** Manual word embeddings, vector visualization, 2D & 3D plotting, cosine similarity  

---

### **3️⃣ UMAP: Visualizing High-Dimensional Word Embeddings**  
✅ Understand **why UMAP is useful** for dimensionality reduction  
✅ Reduce **100D word embeddings → 2D** using UMAP  
✅ **Visualize word relationships** in an interactive plot  
✅ Experiment with **custom datasets and high-dimensional words**  

📌 **Key Skills:** UMAP, dimensionality reduction, high-dimensional word embeddings  

---

## **💡 How to Use These Lessons?**  
📌 Run the Python scripts in **Jupyter Notebook** or a Python environment  
📌 Modify the examples, **try different words**, and **experiment with your own datasets**  
📌 Explore how words relate and **see them in action!**  

🚀 **Have fun exploring the magic of words and vectors!** 🚀
